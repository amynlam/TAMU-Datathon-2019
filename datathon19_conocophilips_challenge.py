# -*- coding: utf-8 -*-
"""Datathon19 ConocoPhilips Challenge

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vFVdPDakuOb83vIVv4UYahrFVTnQA7_M
"""

import pandas as pd
import numpy as np
import io #for importing data sets direclty from personal computer directory 


# downloadable data from: https://www.kaggle.com/c/equipfails/overview/description
# **uncomment two lines below in order to import data set from local files
#from google.colab import files 
#uploaded = files.upload()

#load data
train_data = pd.read_csv("equip_failures_training_set.csv")
train_data.head()

test_data = pd.read_csv("equip_failures_test_set.csv")
test_data.head()

# replace na's and nan's with 0 as opposed to mean, median or mode (because using these would assume some sort of dependency among failures or sensor readings, 
# which is not accurate in this scenario)

#FIXME: implement removeNAs function to provide robustness with any input 
def removeNAs(df):
  df_remove_na=df.replace("na",0)
  np.nan_to_num(0)
  df_remove_na.head()
  return df_remove_na

#clean data 
train_data=train_data.replace("na",0)
np.nan_to_num(0)
train_data.head()

test_data=test_data.replace("na",0)
np.nan_to_num(0)
test_data.head()

from sklearn.linear_model import LogisticRegression
from sklearn import preprocessing

#seperate features and target of training data
y_train=train_data[train_data.columns[1]]
x_train=train_data.drop(columns=['target'])

# Get column names first
names = x_train.columns
# Create the scaler object (basically define how we will scale data; we will do this by normalizing data to the standard normal distribution)
# Reason to do this is because when iterating to try and have the Sigmoid function converge, it  takes a while (adjusting max_iter para in LogisticRegression function)
# So, we scale the data down to make the needed interations required for convergence lower (and therfore lowering run-time)
scaler = preprocessing.StandardScaler()
# Fit the data on the scaler object
scaled_df = scaler.fit_transform(x_train)
scaled_df = pd.DataFrame(scaled_df, columns=names)
train_data_scaled=scaled_df

#re-assign scaled data to x_train
x_train=pd.DataFrame(standardized_train_x)

#create the LogisticRegression object
logmodel = LogisticRegression(solver="lbfgs", max_iter=10000)
logmodel

#Scaling the data causes features to become numerical data (floating points), and logistic regression requires categorical data as inputs, so 
# LabelEncoder() basically converts all floating points to categorical by mapping each unique floating point to a unique integer (interpretted as categorical type)
lab_enc = preprocessing.LabelEncoder()
training_scores_encoded = lab_enc.fit_transform(y_train)
logmodel.fit(x_train, training_scores_encoded) #fit our model

from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split


#to test accuracy, we take a portion of training data to use as testing data because testing data provided does not have actual target recorded ( to compare to), so we 
# take a subset of the training data as "testing" in order to test accuracy
x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=100)
predictions = logmodel.predict(x_test)

#see accuracy and f1-mean score
print(classification_report(y_test,predictions))